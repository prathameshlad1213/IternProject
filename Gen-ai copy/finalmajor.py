# -*- coding: utf-8 -*-
"""finalmajor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1haFZ9Zk52K3R5LtZcftwFX6BZBKowJ2r
"""
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

import subprocess

subprocess.run(["pip", "install", "--quiet", "langchain-text-splitters", 
                "langchain-community", "langgraph", "langchain-openai", 
                "langchain-core", "PyPDF2"], check=True)

import os
import getpass
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langgraph.graph import START, StateGraph
from langchain import hub
from langchain_core.documents import Document
from typing_extensions import List, TypedDict

# Set OpenAI API Key
import os
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Get OpenAI API Key from environment variables
api_key = os.getenv("OPENAI_API_KEY")

# Check if API key is available
if api_key:
    os.environ["OPENAI_API_KEY"] = api_key
else:
    raise ValueError("OpenAI API Key not found. Please add it to the .env file.")


pdf_path = "/Users/prathamesh/Desktop/Gen-ai copy/PEC_DL_Theroy_merged.pdf"

import warnings
import PyPDF2

# Suppress PDF warnings
warnings.filterwarnings("ignore", category=UserWarning)
PyPDF2.PdfReader.strict = False  # Disable strict parsing

from langchain_community.document_loaders import PyPDFLoader

try:
    # Open PDF in read-binary mode and pre-process
    with open(pdf_path, "rb") as file:
        pdf_reader = PyPDF2.PdfReader(file, strict=False)  # Looser parsing
        num_pages = len(pdf_reader.pages)
        print(f"Preprocessing: PDF has {num_pages} pages.")

    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    print(f"Loaded document with {len(docs[0].page_content)} characters.")

except Exception as e:
    print(f"Error loading PDF: {e}")


# print(f"Loaded document with {len(docs[0].page_content)} characters.")

import os

# List files in the directory
# print(os.listdir("/Users/prathamesh/Desktop/Gen-ai"))  # Change path as needed

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Path to the uploaded PDF file
pdf_path = "/Users/prathamesh/Desktop/Gen-ai copy/PEC_DL_Theroy_merged.pdf"

# Load the PDF
loader = PyPDFLoader(pdf_path)
docs = loader.load()
print(f"Loaded document with {len(docs[0].page_content)} characters.")

# Split the document into smaller chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
all_splits = text_splitter.split_documents(docs)
# print(f"Number of chunks: {len(all_splits)}")

from langchain_openai import OpenAIEmbeddings
from langchain_core.vectorstores import InMemoryVectorStore

# Initialize embeddings and vector store
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
vector_store = InMemoryVectorStore(embeddings)

# Add chunks to the vector store
_ = vector_store.add_documents(documents=all_splits)
# print("Document chunks embedded and added to vector store.")

from langchain import hub
from langchain_core.documents import Document
from langgraph.graph import START, StateGraph
from typing_extensions import List, TypedDict
from langchain_openai import ChatOpenAI

# Define application state
class State(TypedDict):
    question: str
    context: List[Document]
    answer: str

# Define retrieval step
def retrieve(state: State):
    retrieved_docs = vector_store.similarity_search(state["question"])
    return {"context": retrieved_docs}

# Define generation step
def generate(state: State):
    docs_content = "\n\n".join(doc.page_content[:500] for doc in state["context"][:3])
    prompt = hub.pull("rlm/rag-prompt")
    messages = prompt.invoke({"question": state["question"], "context": docs_content})
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, max_tokens=50)
    response = llm.invoke(messages)
    return {"answer": response.content}

# Build and compile the graph
graph_builder = StateGraph(State).add_sequence([retrieve, generate])
graph_builder.add_edge(START, "retrieve")
graph = graph_builder.compile()
# print("RAG workflow defined and compiled.")

# Example question
question = "syllabus of deep learning?"

# Get the response
response = graph.invoke({"question": question})
print("Answer:", response["answer"])

"""**extends the implementation to accommodate conversation-style interactions and multi-step retrieval processes**"""

from typing import List, TypedDict
from langchain_core.documents import Document

class ConversationState(TypedDict):
    history: List[dict]  # List of {"user": str, "assistant": str}
    question: str
    context: List[Document]
    answer: str

def coarse_retrieve(state: ConversationState):
    """Perform a broad retrieval to get relevant documents."""
    broad_docs = vector_store.similarity_search(state["question"], k=10)  # Retrieve top 10
    return {"context": broad_docs}

def fine_retrieve(state: ConversationState):
    """Refine the retrieved documents for more relevance."""
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    # Perform a second similarity search within the broad set
    refined_docs = vector_store.similarity_search(state["question"], k=5, context=docs_content)
    return {"context": refined_docs}

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

def generate(state: ConversationState):
    """Generate an answer using the refined documents and conversation history."""
    docs_content = "\n\n".join(doc.page_content for doc in state["context"])
    history_content = "\n".join([f"User: {entry['user']}\nAssistant: {entry['assistant']}" for entry in state["history"]])

    prompt = (
        f"Conversation history:\n{history_content}\n\n"
        f"User question: {state['question']}\n\n"
        f"Context from retrieved documents:\n{docs_content}\n\n"
        "Answer the question in detail and maintain a conversational tone."
    )

    response = llm.invoke([{"role": "system", "content": prompt}])
    return {"answer": response.content}

from langgraph.graph import START, StateGraph

graph_builder = StateGraph(ConversationState).add_sequence([
    coarse_retrieve,  # Coarse retrieval
    fine_retrieve,    # Fine retrieval
    generate          # Generate response
])
graph_builder.add_edge(START, "coarse_retrieve")
graph = graph_builder.compile()

# Initialize conversation state
conversation_state = {"history": [], "question": "", "context": [], "answer": ""}

def chat():
    global conversation_state
    print("Assistant: Hi! How can I help you today?")

    while True:
        user_input = input("You: ")
        if user_input.lower() in ["exit", "quit"]:
            print("Assistant: Goodbye!")
            break

        # Update the conversation state with the new question
        conversation_state["question"] = user_input

        # Invoke the state graph to get a response
        response = graph.invoke(conversation_state)

        # Update the conversation history
        conversation_state["history"].append({"user": user_input, "assistant": response["answer"]})

        # Display the answer
        print(f"Assistant: {response['answer']}")

chat()

